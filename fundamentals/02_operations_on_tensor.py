"""
1) Aritmetik Ä°ÅŸlemler
2) In-place operations
3) Matrix Multiplication (Matris Ã§arpÄ±mÄ±)
4) Tensor Indexing and Slicing
5) Reshaping and Views
6) Combining and Splitting Tensors
"""
import torch

print("===================================== 1) Aritmetik Ä°ÅŸlemler: ")

"""
    PyTorch tensor'larÄ± Ã¼zerinde NumPy array'leri gibi matematiksel iÅŸlemler yapÄ±labilir (element-wise operations).
"""

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])

print(x + y)  # element-wise addition
print(x * y)  # element-wise multiplication
print(x / y)  # element-wise division (float dÃ¶ner)
print(x - y)  # element-wise subtraction

"""
    Bu iÅŸlemler function Ã§aÄŸrÄ±larÄ±yla da yapÄ±labilir; ikisi eÅŸdeÄŸerdir.
    Function versiyonunda output parametresi verilebilir (out=...).
"""

torch.add(x, y)
torch.sub(x, y)
torch.mul(x, y)
torch.div(x, y)

print("===================================== 2) In-place operations: ")

"""
    In-place operation: iÅŸlem sonucu yeni bir tensor'a yazÄ±lmak yerine mevcut tensor'un Ã¼stÃ¼ne yazÄ±lÄ±r (mutates in place).
    PyTorch'ta in-place operasyonlarÄ±n sonuna '_' konur.
    Dikkat: in-place iÅŸlemler autograd (automatic differentiation) sÄ±rasÄ±nda gradient hesaplarÄ±nÄ± bozabilir.
"""

print(x)
x.add_(y)     # x = x + y (in-place)
print(x)

print("===================================== 3) Matrix Multiplication: ")

"""
    PyTorch'ta '*' operatÃ¶rÃ¼ element-wise Ã§arpar. GerÃ§ek matris Ã§arpÄ±mÄ± (matrix multiplication) iÃ§in
    '@' operatÃ¶rÃ¼ veya torch.matmul() kullanÄ±lÄ±r.
"""
a = torch.tensor([[1, 2], [4, 5]])
b = torch.tensor([[6, 7], [8, 9]])

print(a @ b)
print(a.matmul(b))
# eÄŸer a ve b 1D vektÃ¶rse -> dot product (torch.dot kullanÄ±labilir)
# eÄŸer a ve b 2D matrisse -> satÄ±r x sÃ¼tun Ã§arpÄ±mÄ±
# eÄŸer tensor'lar 3D+ ise -> batch matrix multiplication (Ã¶r: (batch_size x n x m) @ (batch_size x m x p))

print("===================================== 4) Tensor Indexing and Slicing: ")

"""
    NumPy'daki gibi PyTorch'ta da tensor'lardan belli elemanlar ve dilimler alÄ±nabilir.
"""
my_tensor = torch.tensor([[1, 2, 3],
                          [4, 5, 6],
                          [7, 8, 9]])
print(my_tensor.size())  # torch.Size([...]); .shape ile aynÄ±dÄ±r

print(my_tensor[0, 0])  # ilk satÄ±r, ilk sÃ¼tun
print(my_tensor[:, 1])  # tÃ¼m satÄ±rlar, 2. sÃ¼tun
print(my_tensor[2, :])  # 3. satÄ±r, tÃ¼m sÃ¼tunlar
# indexing -> tek bir eleman seÃ§mek
# slicing  -> bir aralÄ±ÄŸÄ±/dilimi seÃ§mek
# ':' tÃ¼m elemanlar demektir

print("===================================== 5) Reshaping and Views: ")

"""
    Bir Tensorâ€™un ÅŸeklini (shape) deÄŸiÅŸtirmek iÃ§in:
        1) reshape()
        2) view()
    Not (reshape vs view):
      - reshape(): mÃ¼mkÃ¼nse view dÃ¶ndÃ¼rÃ¼r; mÃ¼mkÃ¼n deÄŸilse kopya (copy) dÃ¶ndÃ¼rebilir. Esnek davranÄ±r.
      - view(): her zaman contiguous (bitiÅŸik) memory ister; deÄŸilse .contiguous() Ã§aÄŸÄ±rman gerekir.
"""

x = torch.arange(9)  # tensor([0,1,2,3,4,5,6,7,8])
x_reshaped = x.reshape(3, 3)
# reshape, memory layout uygunsa sadece "gÃ¶rÃ¼nÃ¼mÃ¼" deÄŸiÅŸtirir (no copy). Uygun deÄŸilse kopya oluÅŸturabilir.

# view Ã¶rneÄŸi:
x = torch.arange(10)
# bazÄ± operasyonlardan sonra tensor contiguous olmayabilir; o durumda:
y = x.contiguous().view(2, 5)  # view contiguous ister
print(y.shape)

print("===================================== 5.1) Contiguous MantÄ±ÄŸÄ±: ")

"""
    "contiguous" bir tensor'un verilerinin bellekte (RAM'de) ardÄ±ÅŸÄ±k (bitiÅŸik) olarak tutulduÄŸu anlamÄ±na gelir.
    Yani elemanlar yan yana bir blok hÃ¢linde depolanÄ±r.

    Non-contiguous ise tensor'un bazÄ± iÅŸlemler (Ã¶rneÄŸin transpose) sonrasÄ± bellekteki sÄ±ralamasÄ±nÄ±n bozulduÄŸu durumlardÄ±r.
    Bu durumda tensor verisini farklÄ± sÄ±rayla okur (stride deÄŸerleri deÄŸiÅŸir) ama fiziksel olarak kopya oluÅŸturmaz.
"""

x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

print("x:\n", x)
print("x.is_contiguous() ->", x.is_contiguous())

# Transpose iÅŸlemi (satÄ±r â†” sÃ¼tun deÄŸiÅŸimi) tensor'u non-contiguous yapar
y = x.t()
print("\ny = x.t():\n", y)
print("y.is_contiguous() ->", y.is_contiguous())

"""
    view() fonksiyonu contiguous tensor'lar Ã¼zerinde Ã§alÄ±ÅŸÄ±r, Ã§Ã¼nkÃ¼ sadece "gÃ¶rÃ¼nÃ¼m" deÄŸiÅŸtirir.
    EÄŸer tensor contiguous deÄŸilse hata verir:
"""
try:
    y.view(3, 2)
except RuntimeError as e:
    print("\nview() hatasÄ±:", e)

"""
    Ã§Ã¶zÃ¼m: .contiguous() Ã§aÄŸÄ±rarak tensor'u bellekte ardÄ±ÅŸÄ±k hÃ¢le getirebiliriz.
    bu iÅŸlem fiziksel bir kopya Ã¼retir ve ardÄ±ndan view() kullanÄ±labilir.
"""
z = y.contiguous()
print("\nYeni z = y.contiguous()")
print("z.is_contiguous() ->", z.is_contiguous())
print("z.view(3, 2):\n", z.view(3, 2))

"""
    reshape() -> otomatik olarak bu kontrolÃ¼ yapar:
        - eÄŸer tensor contiguous ise: view() gibi davranÄ±r (no copy)
        - deÄŸilse: contiguous() + view() kombinasyonu yapar (gerekirse copy)
    bu yÃ¼zden reshape() daha esnektir, view() ise daha hÄ±zlÄ±dÄ±r ama sÄ±nÄ±rlÄ±dÄ±r.
"""

x = torch.arange(10)
print("\nreshape Ã¶rneÄŸi:", x.reshape(2, 5))
print("view Ã¶rneÄŸi:", x.contiguous().view(2, 5))

"""
    Ã–zet Tablo:
        - reshape(): gerekirse kopya oluÅŸturur â†’ gÃ¼venli ve esnek
        - view(): contiguous zorunluluÄŸu vardÄ±r â†’ hÄ±zlÄ± ama sÄ±nÄ±rlÄ±
        - .contiguous(): tensor'u bellekte yeniden dÃ¼zenleyerek view() ile uyumlu hale getirir
"""


print("===================================== 6) Combining and Splitting Tensors: ")

"""
    Tensor'larÄ± birleÅŸtirmek (concatenate) ve bÃ¶lmek (split) iÃ§in:
"""
x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6], [7, 8]])

print(torch.cat((x, y), dim=0))  # dim=0 -> satÄ±r yÃ¶nÃ¼nde birleÅŸtir
print(torch.cat((x, y), dim=1))  # dim=1 -> sÃ¼tun yÃ¶nÃ¼nde birleÅŸtir

t = torch.arange(10)
print(torch.chunk(t, 3))  # chunk(n_chunks): t'yi ~eÅŸit 3 parÃ§aya bÃ¶ler (son parÃ§a daha kÄ±sa olabilir)
print(torch.split(t, 3))  # split(split_size): her parÃ§anÄ±n boyutu 3 olacak ÅŸekilde bÃ¶ler (son parÃ§a daha kÄ±sa olabilir)
# Ek not:
#   - torch.chunk(input, chunks)   -> parÃ§a sayÄ±sÄ±nÄ± verirsin.
#   - torch.split(input, split_size_or_sections) -> parÃ§a boyutunu verirsin ya da [boyutlar] listesi verebilirsin.

print("===================================== *) Hot Takes: ")

"""
    1) Interoperability with NumPy:
       torch_tensor.numpy() -> NumPy array ile aynÄ± memory'yi paylaÅŸÄ±r (shared memory).
       torch.from_numpy(ndarray) -> yine shared memory; ndarray deÄŸiÅŸirse tensor da etkilenir.
       Tam baÄŸÄ±msÄ±z kopya istiyorsan torch.tensor(ndarray) kullan.
"""

"""
    2) Broadcasting:
       FarklÄ± boyutlu tensor'lar, broadcasting kurallarÄ±yla otomatik geniÅŸletilerek (virtually expanded) element-wise iÅŸlem
       yapÄ±labilir. Bir eksen boyutu 1 ise diÄŸerine "yayÄ±lÄ±r" (expand) kurala uygun olduÄŸu sÃ¼rece.
"""
x = torch.ones(2, 1)
y = torch.ones(1, 5)
print((x + y).shape)  # -> torch.Size([2, 5])
print(x + y)

print("===================================== ğŸ“˜ Summary Table: Tensor Operations")

"""
| Konsept | AÃ§Ä±klama | Ã–rnek Kod |
|----------|-----------|-----------|
| element-wise operations | Her eleman kendi karÅŸÄ±lÄ±ÄŸÄ±yla iÅŸlem gÃ¶rÃ¼r. | x * y, torch.add(x, y) |
| in-place operation | Mevcut tensor'u deÄŸiÅŸtirir, yeni kopya oluÅŸturmaz. | x.add_(y) |
| matrix multiplication | SatÄ±r x sÃ¼tun Ã§arpÄ±mÄ± yapar. | a @ b, torch.matmul(a, b) |
| indexing / slicing | Belirli eleman veya aralÄ±k seÃ§imi. | tensor[:, 1], tensor[0, 0] |
| reshape / view | Tensor'un ÅŸeklini deÄŸiÅŸtirir (view contiguous ister). | x.reshape(3, 3), x.view(2, 5) |
| contiguous | Tensor'un bellekte ardÄ±ÅŸÄ±k olma durumu. | x.is_contiguous(), x.contiguous() |
| concatenate / split | Tensor'larÄ± birleÅŸtirir veya bÃ¶ler. | torch.cat(), torch.split() |
| interoperability | NumPy ve PyTorch objeleri bellek paylaÅŸabilir. | torch.from_numpy(a), t.numpy() |
| broadcasting | BoyutlarÄ± farklÄ± tensor'larÄ± otomatik geniÅŸletir. | x + y (farklÄ± shape'lerde) |
"""
