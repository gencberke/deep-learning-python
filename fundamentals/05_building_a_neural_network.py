"""
1) Neural Network nedir?
2) Model, Layer ve parameter kavramlarÄ±
3) Toy dataset oluÅŸturmak
4) Forward ve Backward akÄ±ÅŸÄ±
5) Loss function
6) Optimizer
7) Training loop
8) Modelin Ã¶ÄŸrendiÄŸini gÃ¶zlemlemek
"""

import torch
from torch import nn

print("===================================== 1) Neural Network Nedir?")

"""
1) Yapay sinir aÄŸlarÄ± (neural networks), birbirine baÄŸlanan basit layer'lardan oluÅŸur.
   Her katman (layer) giriÅŸte aldÄ±ÄŸÄ± tensor'u weight ve bias deÄŸerleriyle iÅŸler
   ve aktivasyon fonksiyonu ile non-linear (doÄŸrusal olmayan) bir dÃ¶nÃ¼ÅŸÃ¼m uygular. 

        y = activation(Wx + b)

   PyTorch'ta bu yapÄ±yÄ± kurmak iÃ§in: 
        1 â†’ Katmanlar (nn.Linear, nn.Conv2d, vs.)
        2 â†’ Aktivasyon fonksiyonlarÄ± (nn.ReLU, nn.Sigmoid, vs.)
        3 â†’ Model tanÄ±mÄ± (nn.Module)
        4 â†’ KayÄ±p fonksiyonu (Loss Function)
        5 â†’ Optimizer (Ã¶r: SGD, Adam)
   Bu bileÅŸenlerin hepsi bir araya geldiÄŸinde Ã¶ÄŸrenebilen bir model oluÅŸur.

   Bu notebookâ€™ta bu kavramlarÄ±n hepsini en basit haliyle uygulayacaÄŸÄ±z.
   KÃ¼Ã§Ã¼k bir â€œtoy datasetâ€ (Ã¶r. y = 2x + 1 gibi) Ã¼zerinde model eÄŸitimi yapacaÄŸÄ±z.
"""

print("===================================== 2) Model, Layer ve Parametre KavramlarÄ±:")

"""
2) Bir model PyTorch'ta nn.Module sÄ±nÄ±fÄ±ndan tÃ¼retilir. 
   Her modelin:
        - KatmanlarÄ± (layers)
        - Parametreleri (weights & biases)
        - forward() metodu vardÄ±r (verinin modelden geÃ§iÅŸi burada tanÄ±mlanÄ±r)
"""


class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(in_features=1, out_features=4)
        self.activation = nn.ReLU()
        self.layer2 = nn.Linear(in_features=4, out_features=1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.layer2(x)
        return x


model = SimpleNN()
print(model)

"""
    nn.Linear â†’ y = xW^T + b iÅŸlemini yapar
    nn.ReLU â†’ negatif deÄŸerleri sÄ±fÄ±rlar, pozitifleri korur
    forward() â†’ verinin modelden nasÄ±l geÃ§tiÄŸini (flow) tanÄ±mlar
"""

print("===================================== 3) Toy Dataset OluÅŸturmak:")

"""
3) KÃ¼Ã§Ã¼k bir veri seti (toy dataset) oluÅŸturacaÄŸÄ±z.
   GerÃ§ek hayattaki veriler kusursuz deÄŸildir, bu yÃ¼zden veriye biraz rastgelelik (noise) ekleyeceÄŸiz.
"""
torch.manual_seed(42)
x = torch.linspace(0, 5, steps=100).unsqueeze(1)
# .unsqueeze(1): tensora yeni bir boyut ekler â†’ nn.Linear (batch, feature) beklediÄŸi iÃ§in gerekli.
# KoymasaydÄ±k linspace bize (100,) boyutunda bir tensor Ã¼retecekti.
# nn.Linear, bunu 100 sample ve 1 feature olarak parametrize eder.

noise = torch.randn_like(x) * 0.2
# GerÃ§ek verilerde hata ve gÃ¼rÃ¼ltÃ¼ (noise) olur.
# Burada Gaussian daÄŸÄ±lÄ±mlÄ± kÃ¼Ã§Ã¼k rastgele gÃ¼rÃ¼ltÃ¼ ekliyoruz.
# EÄŸer hiÃ§ noise eklemezsek, model lineer fonksiyonu Ã§ok kolay ezberler (0 training effort).

y = 2 * x + 1 + noise
# Bu satÄ±r bizim hedef (target) deÄŸerlerimizi oluÅŸturuyor: y = 2x + 1 + noise

print("===================================== 4) Forward ve Backward AkÄ±ÅŸÄ±:")

"""
4) Modelin Ã§Ä±ktÄ±sÄ±nÄ± hesaplayalÄ±m:
"""
y_prediction = model(x)
print(y_prediction[:5])

"""
    PyTorch tÃ¼m parametreleri autograd sistemiyle takip eder.
"""
for name, param in model.named_parameters():
    print(name, param.shape, param.requires_grad)

"""
    Bu parametreler (weight, bias) eÄŸitim sÃ¼resince gÃ¼ncellenecek.
    forward() â†’ inputâ€™tan outputâ€™a
    backward() â†’ lossâ€™tan parametrelere (gradient hesaplama)
"""

print("===================================== 4.1) Weight ve Bias KavramÄ±:")

"""
    Bir yapay sinir aÄŸÄ± (neural network) katmanÄ±nda iki temel Ã¶ÄŸrenilebilir parametre vardÄ±r:
        1) weight (aÄŸÄ±rlÄ±k)
        2) bias (sapma deÄŸeri)

    Matematiksel olarak bir lineer katman (nn.Linear) ÅŸu iÅŸlemi yapar:
        y = W * x + b

    Buradaki:
        - W (weight): giriÅŸteki her bir feature'Ä±n Ã§Ä±ktÄ± Ã¼zerindeki etkisini belirler.
                      Yani modelin 'eÄŸimi'dir (hangi yÃ¶n ve bÃ¼yÃ¼klÃ¼kte etkili olduÄŸunu belirler).
        - b (bias): fonksiyonu yukarÄ±/aÅŸaÄŸÄ± kaydÄ±ran sabit deÄŸerdir.
                    EÄŸer bias olmasa, model sadece orijinden (0,0) geÃ§en doÄŸrular Ã¶ÄŸrenebilirdi.
                    Bias fonksiyonu esnekleÅŸtirir.

    EÄŸitim sÃ¼recinde model, weight ve bias deÄŸerlerini loss fonksiyonunu minimize edecek ÅŸekilde
    gÃ¼nceller. Bu sayede model veriye daha uygun hale gelir.

    Her nn.Linear layer'Ä± otomatik olarak ÅŸu parametreleri taÅŸÄ±r:
        - weight -> shape = (out_features, in_features)
        - bias   -> shape = (out_features,)
"""
y_prediction = model(x)
print(y_prediction[:5])

for name, param in model.named_parameters():
    print(name, param.shape, param.requires_grad)

"""
    Ã‡Ä±ktÄ± yorumlama:
    -------------------------------------
    â–ª layer1.weight -> (4, 1) â†’ 1 input, 4 nÃ¶ron
    â–ª layer1.bias   -> (4,)   â†’ her nÃ¶ronun 1 bias'Ä± var
    â–ª layer2.weight -> (1, 4) â†’ 4 input alÄ±p 1 Ã§Ä±ktÄ± Ã¼retir
    â–ª layer2.bias   -> (1,)   â†’ tek Ã§Ä±ktÄ± iÃ§in tek bias
    â–ª tensor(..., grad_fn=<SliceBackward0>) â†’ bu deÄŸerler autograd graph'Ä±na baÄŸlÄ±dÄ±r, tÃ¼rev alÄ±nabilir.

    EÄŸitim boyunca:
        - weight ve bias rastgele baÅŸlatÄ±lÄ±r (random initialization).
        - loss.backward() Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda bu parametrelerin .grad deÄŸerleri hesaplanÄ±r.
        - optimizer.step() bu parametreleri gradient doÄŸrultusunda gÃ¼nceller.
        - BÃ¶ylece model yavaÅŸ yavaÅŸ doÄŸru fonksiyonu Ã¶ÄŸrenir.
"""

print("===================================== 5) Loss Function:")

"""
5) Loss, tahmin edilen deÄŸerle gerÃ§ek deÄŸer arasÄ±ndaki farkÄ± Ã¶lÃ§er.
   Modelin ana amacÄ± bu farkÄ± minimize etmektir.
"""
loss_fn = nn.MSELoss()  # Mean Squared Error

y_prediction = model(x)
loss = loss_fn(y_prediction, y)
print("Initial Loss:", loss)

# Loss function ileride daha detaylÄ± ele alÄ±nacak.
# BaÅŸlangÄ±Ã§ta lossâ€™un yÃ¼ksek olmasÄ± anormal deÄŸildir.
# Ã–nemli olan epoch'lar ilerledikÃ§e lossâ€™un dÃ¼zenli olarak azalmasÄ±dÄ±r.

print("===================================== 6) Optimizer:")

"""
6) Optimizer gradient'leri kullanarak parametreleri gÃ¼nceller.
   Yani modelin 'Ã¶ÄŸrenme mekanizmasÄ±'dÄ±r.
"""
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# SGD (Stochastic Gradient Descent) â†’ klasik gradient descent algoritmasÄ±.
# lr (learning rate): adÄ±m boyu, ne kadar bÃ¼yÃ¼kse o kadar hÄ±zlÄ± ama riskli Ã¶ÄŸrenme.

"""
optimizer.step():
    -> backward() ile hesaplanan gradient'leri kullanarak parametreleri gÃ¼nceller.
    -> Yani weight ve bias deÄŸerleri loss'u azaltacak yÃ¶nde deÄŸiÅŸtirilir.

optimizer.zero_grad():
    -> Gradient belleÄŸini temizler.
    -> backward() Ã§aÄŸrÄ±sÄ± her seferinde mevcut gradientleri eklediÄŸi iÃ§in
       Ã¶nceki iterasyondan kalan gradientler sÄ±fÄ±rlanmazsa birikir (accumulate).
    -> Bu yÃ¼zden her epoch baÅŸÄ±nda gradientleri sÄ±fÄ±rlarÄ±z.
"""

print("===================================== 7) Training Loop:")

"""
7) Modeli eÄŸitmek iÃ§in her epoch'ta aynÄ± iÅŸlemler tekrarlanÄ±r.
"""
epochs = 100
for epoch in range(epochs):
    # 1) forward
    y_prediction = model(x)

    # 2) loss hesapla
    loss = loss_fn(y_prediction, y)

    # 3) backward
    loss.backward()

    # 4) parametreleri gÃ¼ncelle
    optimizer.step()

    # 5) gradientleri sÄ±fÄ±rla
    optimizer.zero_grad()

    if (epoch + 1) % 40 == 0:
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}")

"""
loss.backward() â†’ gradient hesaplar
optimizer.step() â†’ parametreleri gÃ¼nceller
optimizer.zero_grad() â†’ eski gradientleri sÄ±fÄ±rlar
"""

print("===================================== 7.1) Epoch KavramÄ±:")

"""
    "Epoch" bir eÄŸitim dÃ¶ngÃ¼sÃ¼nÃ¼n tamamÄ±nÄ± tanÄ±mlar.

    â–ª TanÄ±m:
        Bir epoch, modelin eÄŸitim verisinin tamamÄ±nÄ± bir kez gÃ¶rÃ¼p
        forward() â†’ backward() â†’ optimizer.step() iÅŸlemlerini tamamladÄ±ÄŸÄ± tam bir turdur.

    â–ª Neden birden fazla epoch?
        Model tek bir turda genellikle yeterince Ã¶ÄŸrenemez Ã§Ã¼nkÃ¼ weight ve bias deÄŸerleri
        her adÄ±mda kÃ¼Ã§Ã¼k miktarlarda gÃ¼ncellenir. 
        Bu nedenle veriyi defalarca gÃ¶rmesi gerekir.
        Her epoch'ta model biraz daha iyi hale gelir, loss genellikle giderek azalÄ±r.

    â–ª Epoch - Iteration - Batch farkÄ±:
        Dataset â†’ eÄŸitimde kullanÄ±lan tÃ¼m Ã¶rnekler (Ã¶r. 1000 sample)
        Batch â†’ bir seferde iÅŸlenen Ã¶rnek grubu (Ã¶r. 100 sample)
        Iteration â†’ her batch'in iÅŸlenmesi (Ã¶r. 1000/100 = 10 iteration)
        Epoch â†’ tÃ¼m dataset'in baÅŸtan sona 1 kez iÅŸlenmesi (10 iteration = 1 epoch)

    â–ª Ã–rnek:
        1000 Ã¶rnek, batch=100 â†’ 1 epoch = 10 iteration
        200 epoch â†’ 200 Ã— 10 = 2000 iteration

    â–ª Epoch sayÄ±sÄ± nasÄ±l seÃ§ilir?
        - Az epoch â†’ model yeterince Ã¶ÄŸrenemez (underfitting)
        - Ã‡ok epoch â†’ model aÅŸÄ±rÄ± Ã¶ÄŸrenir (overfitting)
        - Uygun epoch sayÄ±sÄ± â†’ lossâ€™un dÃ¼ÅŸÃ¼p sabitlendiÄŸi nokta

        Basit linear toy model â†’ genelde 100â€“300 epoch yeterlidir.
        Daha karmaÅŸÄ±k modellerde (CNN, RNN) epoch sayÄ±sÄ± artar.

    â–ª Loss - Epoch iliÅŸkisi (Ã¶rnek):
            Epoch 1   | Loss: 31.2060
            Epoch 40  | Loss: 4.1224
            Epoch 80  | Loss: 1.0820
            Epoch 120 | Loss: 0.3821
            Epoch 160 | Loss: 0.1563
            Epoch 200 | Loss: 0.0897

        Loss azaldÄ±kÃ§a model veriye daha Ã§ok yaklaÅŸÄ±r.
        Bu azalan ama stabilize olan loss eÄŸrisi modelin doÄŸru Ã¶ÄŸrendiÄŸini gÃ¶sterir.

    â–ª Ã–zetle:
        - Epoch â†’ modelin tÃ¼m veriyi baÅŸtan sona bir kez iÅŸlemesi
        - Iteration â†’ bir batch'in iÅŸlenmesi
        - Batch â†’ bir seferde kullanÄ±lan Ã¶rnek sayÄ±sÄ±
        - Ã‡ok az epoch â†’ underfitting
        - Ã‡ok fazla epoch â†’ overfitting
        - Epoch arttÄ±kÃ§a loss azalÄ±r ama bir noktadan sonra sabitlenir (convergence)

    ğŸ’¡ KÄ±sa tanÄ±m:
        Epoch, modelin tÃ¼m eÄŸitim verisini bir kez iÅŸleyip aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncellediÄŸi dÃ¶ngÃ¼dÃ¼r.
        EÄŸitim genellikle onlarca veya yÃ¼zlerce epoch sÃ¼rer, her seferinde loss biraz daha azalÄ±r.
"""

print("===================================== 8) Modelin Ã–ÄŸrendiÄŸini GÃ¶zlemlemek:")

with torch.no_grad():
    y_eval = model(x)

"""
Loss azalÄ±yor â†’ model doÄŸru yÃ¶nde Ã¶ÄŸreniyor.
Model artÄ±k yaklaÅŸÄ±k olarak y â‰ˆ 2x + 1 fonksiyonunu Ã¶ÄŸrenmiÅŸ durumda.
"""

print("===================================== *) Hot-Takes:")

"""
- nn.Module -> PyTorch'taki tÃ¼m modellerin temelidir.
- forward() -> inputâ€™tan outputâ€™a veri akÄ±ÅŸÄ±.
- backward() -> autograd sistemiyle gradient hesaplama.
- optimizer.step() -> parametreleri gÃ¼nceller.
- zero_grad() -> gradient belleÄŸini temizler.
- loss.backward() + optimizer.step() dÃ¶ngÃ¼sÃ¼ â†’ â€œlearningâ€.
- Bu yapÄ± tÃ¼m training loopâ€™larÄ±n temel iskeletidir.
"""

print("===================================== ğŸ“˜ Summary Table: Neural Network on Toy Dataset")

"""
| Konsept | AÃ§Ä±klama | Ã–rnek Kod |
|----------|-----------|-----------|
| nn.Module | Model yapÄ±sÄ± tanÄ±mlamak iÃ§in PyTorch sÄ±nÄ±fÄ± | class Model(nn.Module): ... |
| nn.Linear | Fully connected layer (W*x + b) | nn.Linear(1,4) |
| forward() | Verinin modeldeki akÄ±ÅŸÄ±nÄ± belirler | def forward(self, x): ... |
| MSELoss | Tahmin ve gerÃ§ek farkÄ±nÄ± Ã¶lÃ§er | nn.MSELoss() |
| optimizer | Gradientâ€™e gÃ¶re aÄŸÄ±rlÄ±klarÄ± gÃ¼nceller | torch.optim.SGD(...) |
| backward() | Gradient hesaplamasÄ±nÄ± baÅŸlatÄ±r | loss.backward() |
| zero_grad() | Gradient belleÄŸini temizler | optimizer.zero_grad() |
| Training Loop | Modelin Ã¶ÄŸrenme sÃ¼reci (forward â†’ loss â†’ backward â†’ update) | for epoch in range(...): ... |
| model.parameters() | Ã–ÄŸrenilebilir tensorâ€™lar (weights & biases) | model.parameters() |
"""
