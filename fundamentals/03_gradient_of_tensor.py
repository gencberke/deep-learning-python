"""
1) What is Autograd?
2) Computation Graph
3) Backward Propagation
4) Gradient Accumulation
"""
import torch
import numpy as np
from torch.onnx.symbolic_opset9 import tensor

print("===================================== 1) What is Autograd?")
"""
1) autograd -> pytorch'un otomatik difransiyel engine'i yani pytorch bir tensor Ã¼zerindeki tÃ¼m iÅŸlemleri bir graph olarak
    trackler ve bu grafiÄŸi kullanarak gradyen yani tÃ¼rev hesaplayabilir. bu iÅŸlemde deep learningte backpropagation'un
    temelidir.
"""
# requires_grad=True demek, bu tensor Ã¼zerindeki iÅŸlemleri autograd sisteminde takip et.
x = torch.tensor([3.0, 4.0], requires_grad=True)
print(x)
y = (x ** 2).sum() # y = 3'Ã¼n karesi + 4'Ã¼n karesi

# ÅŸimdi burada y'ye gÃ¶re x'in tÃ¼revi hesaplanÄ±r.
y.backward()
print(x.grad)

print("===================================== 2) Computation Graph:")

"""
2) Autograd sistemi her bir iÅŸlemi bir node olarak dÃ¼ÅŸÃ¼nÃ¼r. sonucu ise bir edge olarak. "computation graph" dinamik olarak
    oluÅŸturulur yani her pass forward iÅŸlemi sÄ±rasÄ±nda tekrar tekrar Ã§izilir. bu sisteme define-by-run denilir
    
        x -> (x^2) -> sum -> y
        
    bu zincirde x girdi, (x^2) operation sonucu oluÅŸan intermediate tensor, y final output. pytorch her adÄ±mÄ± takip eder.
    ve backward Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda chain rule'a tÃ¼rev uygular.  
"""

print("===================================== 2.1) Chain Rule (Zincir KuralÄ±): ")

"""
2.1) Chain Rule (Zincir KuralÄ±):

    Matematikte chain rule, bileÅŸik fonksiyonlarÄ±n tÃ¼revini alÄ±rken kullanÄ±lÄ±r.
    EÄŸer y = f(g(x)) ise:
        dy/dx = (dy/dg) * (dg/dx)

    PyTorch'un autograd sistemi de aynen bu mantÄ±kla Ã§alÄ±ÅŸÄ±r.
    Backward propagation sÄ±rasÄ±nda son Ã§Ä±ktÄ±nÄ±n (loss) gradient'i en baÅŸtaki input tensor'lara kadar
    zincirleme olarak (reverse mode differentiation) hesaplanÄ±r.

    Ã–rnek:
        x = 2
        y = x^2
        z = 3y + 5

        z = 3(x^2) + 5
        dz/dx = 3 * 2x = 6x

    PyTorch bu iÅŸlemleri manuel yazmadan otomatik olarak yapar:
"""
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
z = 3 * y + 5
z.backward()
print("x.grad:", x.grad)  # 6x = 12.0

print("===================================== 2.2) non scalar outputs ve gradient arguments: ")

"""
2.2) .backward() metodu yalnÄ±zca skaler sonuÃ§lar iÃ§in Ã§alÄ±ÅŸÄ±r. 
    EÄŸer sonuÃ§ bir vektÃ¶rse (Ã¶rneÄŸin [y1, y2, y3]), hangi yÃ¶n boyunca tÃ¼rev alÄ±nacaÄŸÄ±nÄ± belirtmen gerekir.
"""
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2 # y = [1.0, 4.0, 9.0]
v = torch.tensor([0.1, 0.01, 0.001]) # custom weights (directional vektor)

y.backward(v)
print(x.grad)

"""
    Burada PyTorch ÅŸu hesabÄ± yapar:

    âˆ‚(vÂ·y)/âˆ‚x = v * 2x

    Jacobian-vector product (JVP) olarak bilinir. Tam tÃ¼rev matrisi yerine, belirli bir vektÃ¶r yÃ¶nÃ¼nde tÃ¼rev alÄ±r 
    bu deep learningâ€™de gradient optimizasyonlarÄ±nÄ± hÄ±zlandÄ±rmak iÃ§in kullanÄ±lÄ±r.
"""

print("===================================== 2.3) detach() ve no_grad() :")

"""
2.3) autograd bazen istemediÄŸimiz tensorlarÄ±da izler. bu durumda iki yÃ¶ntem vardÄ±r:
"""
# .detach() -> tensor'u graph'tan ayÄ±rÄ±r ama veriyi korur
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
z = y.detach()  # artÄ±k z'nin requires_grad=False
print(z.requires_grad)  # False

# torch.no_grad() -> context manager olarak Ã§alÄ±ÅŸÄ±r ve iÃ§erideki hiÃ§bir iÅŸlem graph'a eklenmez.
x = torch.tensor([3.0], requires_grad=True)
with torch.no_grad():
    y = x ** 2
print(y.requires_grad)  # False

"""
        .detach() â†’ tensor seviyesinde koparma
        no_grad() â†’ kod bloÄŸu seviyesinde geÃ§ici olarak autogradâ€™Ä± kapatma

    Model evaluation (Ã¶rneÄŸin test setinde) sÄ±rasÄ±nda performans iÃ§in no_grad kullanmak best practiceâ€™tir. (Ã§ok mantÄ±klÄ±)
"""

print("===================================== 3) Gradient Accumulation: ")

"""
3) .backward() her Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda gradient'ler var olan .grad Ã¼zerine eklenir (accumulate) 
    Bu bazen eÄŸitim dÃ¶ngÃ¼lerinde â€œgradient accumulationâ€ olarak bilerek kullanÄ±lÄ±r, ama genellikle temizlemek gerekir. 
"""
x = torch.tensor([2.0], requires_grad=True)
y1 = x ** 2
y2 = x * 3
y1.backward()

print(x.grad) # 4.0

y2.backward()
print(x.grad) # 4.0 + 3.0 = 7.0

x.grad.zero_() # gradyantlarÄ± sÄ±fÄ±rlamak iÃ§in
print(x.grad)

"""
        .zero_() bir in-place operationâ€™dÄ±r, gradient belleÄŸini temizler.

    Optimizerâ€™larda (optimizer.zero_grad()) da bu mantÄ±k kullanÄ±lÄ±r.
"""

print("===================================== 3.1) requires_grad_() ve leaf tensors :")

"""
3.1) Her tensor oluÅŸturulduÄŸunda varsayÄ±lan olarak requires_grad=False gelir.
    Bunu sonradan aktif etmek istersen .requires_grad_() fonksiyonunu kullanabilirsin. 
"""
a = torch.ones(3)
print(a.requires_grad)  # False
a.requires_grad_()      # inplace olarak deÄŸiÅŸtirir
print(a.requires_grad)  # True

"""
Ã–nemli Not: Bir tensor autograd graphâ€™Ä±nda doÄŸrudan kullanÄ±cÄ± tarafÄ±ndan oluÅŸturulmuÅŸ ve bir operation sonucu deÄŸilse, 
            ona leaf tensor denir. Gradientâ€™ler sadece leaf tensorâ€™lar iÃ§in saklanÄ±r (.grad attributeâ€™u yalnÄ±zca onlarda
            dolu olur).
"""

print("===================================== *) Hot-takes ")

"""
1) bir gradyan hesaplamasÄ±nÄ± temporarily durdurmak iÃ§in
"""
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
with torch.no_grad():
    z = y * 3
print(z.requires_grad)  # False

# veya
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y.detach() * 3  # sadece y'yi kopardÄ±k

"""
2) Gradients for multi-step functions:
    Autograd chain rule sayesinde Ã§ok adÄ±mlÄ± fonksiyonlarda da otomatik tÃ¼rev alÄ±r:
"""
x = torch.tensor(3.0, requires_grad=True)
y = x ** 2 + 2 * x + 1  # y = x^2 + 2x + 1
y.backward()
print(x.grad)  # 2x + 2 = 8

print("===================================== ğŸ“˜ Summary Table: Autograd Basics")

"""
| Konsept | AÃ§Ä±klama | Ã–rnek Kod |
|----------|-----------|-----------|
| requires_grad | PyTorch'un gradient hesaplamasÄ±nÄ± aktif eder. | x = torch.tensor([2.0], requires_grad=True) |
| backward() | Scalar bir output'un tÃ¼revini hesaplar. | y.backward() |
| grad | Hesaplanan tÃ¼revi dÃ¶ndÃ¼rÃ¼r. | print(x.grad) |
| computation graph | TÃ¼m iÅŸlemlerin zincir olarak izlendiÄŸi dinamik yapÄ±. | x â†’ xÂ² â†’ sum â†’ y |
| chain rule | BileÅŸik fonksiyonlarda tÃ¼revi zincirleme uygular. | dy/dx = (dy/dg)*(dg/dx) |
| Jacobian-vector product (JVP) | Non-scalar sonuÃ§larda yÃ¶n vektÃ¶rÃ¼ne gÃ¶re tÃ¼rev alÄ±r. | y.backward(v) |
| detach() | Tensor'u graph'tan ayÄ±rÄ±r (gradient takibi durur). | z = y.detach() |
| no_grad() | GeÃ§ici olarak gradient takibini kapatÄ±r. | with torch.no_grad(): ... |
| gradient accumulation | backward() Ã§aÄŸrÄ±larÄ± gradyanlarÄ± toplar (eklenir). | x.grad.zero_() ile temizlenir |
| leaf tensor | DoÄŸrudan kullanÄ±cÄ± tarafÄ±ndan oluÅŸturulmuÅŸ tensor. | a = torch.ones(3, requires_grad=True) |
"""
