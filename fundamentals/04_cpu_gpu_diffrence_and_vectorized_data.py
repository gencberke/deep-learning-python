"""
1) NumPy vs PyTorch - Speed Comparison
2) Vectorized Computation
3) GPU (CUDA) Execution Theory
"""
import numpy as np
import torch
import time

print("===================================== 1) NumPy vs PyTorch - CPU Speed Comparison:")

"""
    NumPy ve PyTorch her ikisi de CPU'da Ã§alÄ±ÅŸabilir. 
    Ancak PyTorch C++ backend (ATen) kullandÄ±ÄŸÄ± iÃ§in bazÄ± matematiksel iÅŸlemleri daha optimize biÃ§imde yÃ¼rÃ¼tÃ¼r.
    Burada CPU Ã¼zerinde basit bir hÄ±z karÅŸÄ±laÅŸtÄ±rmasÄ± yapacaÄŸÄ±z.
"""

# boyut (ne kadar bÃ¼yÃ¼k, o kadar fark)
size = 10_000_000

# NumPy versiyonu
a_np = np.random.rand(size)
b_np = np.random.rand(size)

start = time.time()
c_np = a_np * b_np  # element-wise Ã§arpma
end = time.time()
print(f"NumPy time: {end - start:.6f} seconds")

# PyTorch versiyonu
a_torch = torch.rand(size)
b_torch = torch.rand(size)

start = time.time()
c_torch = a_torch * b_torch  # element-wise Ã§arpma
end = time.time()
print(f"PyTorch (CPU) time: {end - start:.6f} seconds")

"""
    Her iki iÅŸlem de CPU'da olsa bile PyTorch genellikle optimize edilmiÅŸ low-level C++ implementasyonu sayesinde
    kÃ¼Ã§Ã¼k farklarla da olsa daha hÄ±zlÄ± olabilir.
    Ancak bu fark genellikle CPU'da Ã§ok belirgin deÄŸildir, asÄ±l fark GPU'da ortaya Ã§Ä±kar.
"""

print("===================================== 2) Vectorized Computation (VektÃ¶rleÅŸtirilmiÅŸ Hesaplama):")

"""
    Python'da klasik for-loop ile hesap yapmak yerine NumPy veya PyTorch'un vektÃ¶rleÅŸtirilmiÅŸ fonksiyonlarÄ±nÄ±
    kullanmak 100 kata kadar performans farkÄ± yaratabilir. (loop overhead ortadan kalkar) (ileride ele alÄ±nacak)

    AÅŸaÄŸÄ±da klasik Python dÃ¶ngÃ¼sÃ¼yle yapÄ±lan bir iÅŸlemi, PyTorch ve NumPy vektÃ¶rleÅŸtirilmiÅŸ haliyle karÅŸÄ±laÅŸtÄ±ralÄ±m:
"""

# saf Python versiyonu (Ã§ok yavaÅŸ)
x_list = [i for i in range(10_000_00)]
start = time.time()
y_list = [i**2 for i in x_list]
print(f"Pure Python time: {time.time() - start:.6f} seconds")

# NumPy vectorized
x_np = np.arange(10_000_00)
start = time.time()
y_np = x_np ** 2
print(f"NumPy vectorized time: {time.time() - start:.6f} seconds")

# PyTorch vectorized
x_torch = torch.arange(10_000_00)
start = time.time()
y_torch = x_torch ** 2
print(f"PyTorch vectorized time: {time.time() - start:.6f} seconds")

"""
    GÃ¶zlem: for-loop â†’ milyonlarca iteration = Python interpreter overhead
            vectorized â†’ C dÃ¼zeyinde iÅŸlem â†’ Ã§ok daha hÄ±zlÄ±
"""

print("===================================== 3) GPU (CUDA) Execution Theory:")

"""
    GPU hÄ±zlandÄ±rmasÄ± (CUDA) teorik olarak ÅŸu ÅŸekilde Ã§alÄ±ÅŸÄ±r:

    1. Tensor'larÄ± GPU belleÄŸine taÅŸÄ±rsÄ±n:
        x = torch.rand(10000, device='cuda')

    2. GPU Ã¼zerinde iÅŸlem yaparsÄ±n (Ã¶rneÄŸin matrix multiplication):
        y = torch.mm(x, x)

    3. GPU asenkron Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in zaman Ã¶lÃ§Ã¼mÃ¼ yaparken torch.cuda.synchronize() eklenir.
       (CPU zamanlamasÄ±yla GPU'nun bitiÅŸ zamanlamasÄ± farklÄ± olabilir.)

    4. SonuÃ§lar CPU'ya geri taÅŸÄ±nabilir:
        y_cpu = y.to('cpu')

    GPU iÅŸlemleri Ã§oklu Ã§ekirdek (thousands of cores) ve paralel floating-point execution sayesinde
    CPU'ya kÄ±yasla 10-100x hÄ±z artÄ±ÅŸÄ± saÄŸlayabilir.

    Ancak bu yalnÄ±zca NVIDIA GPU + CUDA driver ortamÄ±nda geÃ§erlidir.
    Intel Iris Pro CUDA desteklemediÄŸi iÃ§in iÅŸlemler CPUâ€™da Ã§alÄ±ÅŸacaktÄ±r.
"""

if torch.cuda.is_available():
    print("âœ… CUDA GPU detected:", torch.cuda.get_device_name(0))
else:
    print("âš ï¸ CUDA not available â€” running on CPU (Intel Iris Pro detected)")

print("===================================== *) Hot-takes:")

"""
    - CPU'da NumPy ve PyTorch hÄ±zlarÄ± genellikle benzerdir, bazen PyTorch az farkla Ã¶nde olabilir.
    - GPU (CUDA) varsa, PyTorch bÃ¼yÃ¼k matrix iÅŸlemlerinde dramatik hÄ±z farkÄ± yaratÄ±r.
    - torch.cuda.synchronize() zaman Ã¶lÃ§Ã¼mÃ¼nde Ã¶nemlidir (GPU iÅŸlemleri asenkron Ã§alÄ±ÅŸÄ±r).
    - Vectorized operations, Python loopâ€™larÄ±ndan kat kat hÄ±zlÄ±dÄ±r.
    - Intel Iris Pro gibi entegre GPU'lar CUDA desteklemez â†’ PyTorch iÅŸlemleri CPU Ã¼zerinde yÃ¼rÃ¼r.
"""

print("===================================== ğŸ“˜ Summary Table: NumPy vs PyTorch Computation Speed")

"""
| Konsept | AÃ§Ä±klama | Ã–rnek Kod |
|----------|-----------|-----------|
| Computation Speed | PyTorch C++ backend (ATen) kullandÄ±ÄŸÄ± iÃ§in NumPy'ye kÄ±yasla bazÄ± iÅŸlemlerde CPUâ€™da daha optimize Ã§alÄ±ÅŸabilir. | a_torch * b_torch |
| Vectorized Computation | DÃ¶ngÃ¼ (loop) yerine toplu iÅŸlemler (C dÃ¼zeyinde) kullanarak 10-100x hÄ±z kazanÄ±mÄ± saÄŸlar. | y = x ** 2 |
| Pure Python vs Vectorized | for-loop yerine NumPy/PyTorch kullanmak bÃ¼yÃ¼k fark yaratÄ±r. | [i**2 for i in x] â†’ x**2 |
| GPU (CUDA) Execution | CUDA destekli GPUâ€™larda iÅŸlemler paralel yÃ¼rÃ¼tÃ¼lÃ¼r; CPUâ€™ya gÃ¶re 10â€“100x daha hÄ±zlÄ± olabilir. | x = torch.rand(1000,1000, device='cuda') |
| torch.cuda.is_available() | Sistemde CUDA destekli GPU olup olmadÄ±ÄŸÄ±nÄ± kontrol eder. | torch.cuda.is_available() |
| torch.cuda.synchronize() | GPU asenkron Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in zaman Ã¶lÃ§Ã¼mÃ¼nde CPU ve GPU senkronizasyonu saÄŸlar. | torch.cuda.synchronize() |
| device placement | Tensorâ€™un CPU veya GPUâ€™da oluÅŸturulacaÄŸÄ± yeri belirler. | torch.device("cuda" if torch.cuda.is_available() else "cpu") |
| Intel Iris Pro durumu | CUDA desteklemediÄŸinden tÃ¼m iÅŸlemler CPU Ã¼zerinde yÃ¼rÃ¼r (GPU hÄ±zlanmasÄ± olmaz). | n/a |
| Performance Insight | KÃ¼Ã§Ã¼k iÅŸlemlerde fark azdÄ±r; bÃ¼yÃ¼k matrislerde PyTorch fark yaratÄ±r. | matrix multiplication test |
"""
